{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ae08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca998e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Data Download and Processing\n",
    "# ==========================================\n",
    "def download_probing_data(task_name=\"past_present\", data_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Downloads a specific probing task from the SentEval repository.\n",
    "    task_name options: 'sent_len', 'wc', 'tree_depth', 'top_const', \n",
    "                       'past_present', 'subj_number', 'obj_number', \n",
    "                       'odd_man_out', 'coord_inv'\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    url = f\"https://raw.githubusercontent.com/facebookresearch/SentEval/main/data/probing/{task_name}.txt\"\n",
    "    file_path = os.path.join(data_dir, f\"{task_name}.txt\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {task_name}...\")\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"Data for {task_name} already exists.\")\n",
    "    return file_path\n",
    "\n",
    "class ProbingDataset(Dataset):\n",
    "    def __init__(self, file_path, partition_filter, tokenizer, max_len=128):\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # SentEval format: \"partition \\t label \\t sentence content...\"\n",
    "        label_set = set()\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 3: continue\n",
    "                \n",
    "                partition = parts[0]\n",
    "                label = parts[1]\n",
    "                text = parts[2]\n",
    "                \n",
    "                if partition == partition_filter:\n",
    "                    self.sentences.append(text)\n",
    "                    self.labels.append(label)\n",
    "                    label_set.add(label)\n",
    "        \n",
    "        # Create a mapping from label string to integer\n",
    "        # Note: In a real scenario, ensure this mapping is consistent across train/test\n",
    "        self.label_map = {l: i for i, l in enumerate(sorted(list(label_set)))}\n",
    "        self.num_classes = len(self.label_map)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.sentences[idx]\n",
    "        label_str = self.labels[idx]\n",
    "        label_id = self.label_map[label_str]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label_id, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501199ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Setup BERT Encoder (The Subject of the Probe)\n",
    "# ==========================================\n",
    "def get_bert_encoder():\n",
    "    print(\"Loading BERT model...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Freeze the encoder weights! We are probing it, not training it.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d413859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Setup Probing Classifier\n",
    "# ==========================================\n",
    "class ProbingClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(ProbingClassifier, self).__init__()\n",
    "        # Standard probing architecture: Linear -> Tanh -> Linear\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6956a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Training and Evaluation Functions\n",
    "# ==========================================\n",
    "def train_probe(encoder, probe, train_loader, device, epochs=5, learning_rate=1e-3):\n",
    "    encoder.to(device)\n",
    "    probe.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(probe.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"Starting training on {device}...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        probe.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Step 1: Get Embeddings (No Gradients for Encoder)\n",
    "            with torch.no_grad():\n",
    "                outputs = encoder(input_ids, attention_mask=mask)\n",
    "                # Use [CLS] token representation (first token)\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :] \n",
    "                \n",
    "            # Step 2: Train Probe\n",
    "            optimizer.zero_grad()\n",
    "            logits = probe(embeddings)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbed3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probe(encoder, probe, test_loader, device):\n",
    "    encoder.to(device)\n",
    "    probe.to(device)\n",
    "    probe.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    print(\"Evaluating...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = encoder(input_ids, attention_mask=mask)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :] # [CLS] token\n",
    "            \n",
    "            logits = probe(embeddings)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Probe Accuracy: {acc*100:.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d94511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "TASK = \"past_present\" # Try: 'sent_len', 'tree_depth'\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 512\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc21528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e24a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading past_present...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "data_path = download_probing_data(TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5701818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507e5b3c60f84d2ab4fe8bf4ffef6a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1135f4fc8d054f6d886da4d3c9399801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4070ffed91471dadf0a98025c58676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19aefe31775c4d8ab6631b20825465f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409c15800fc140c5b44efa903c134975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, encoder = get_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6842c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must ensure the label mapping is consistent. \n",
    "# In this simple script, we initialize the dataset twice, which isn't ideal \n",
    "# but works if the data file contains all labels in both splits.\n",
    "# Ideally, build the label_map once from the whole file.\n",
    "train_ds = ProbingDataset(data_path, 'tr', tokenizer)\n",
    "test_ds = ProbingDataset(data_path, 'te', tokenizer)\n",
    "\n",
    "# Sync label maps (just in case)\n",
    "test_ds.label_map = train_ds.label_map \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e3991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: past_present\n",
      "Classes: {'PAST': 0, 'PRES': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Classes: {train_ds.label_map}\")\n",
    "\n",
    "# 2. Setup Probe\n",
    "# Input dim for BERT-base is 768\n",
    "probe = ProbingClassifier(768, HIDDEN_DIM, train_ds.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd05ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 782/782 [03:05<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 782/782 [03:04<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 782/782 [03:04<00:00,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.2641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Train\n",
    "train_probe(encoder, probe, train_loader, DEVICE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569aabb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 79/79 [00:18<00:00,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe Accuracy: 88.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8829"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluate\n",
    "evaluate_probe(encoder, probe, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32999724",
   "metadata": {},
   "source": [
    ".8829 accuracy on past present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2eea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for sentence_length already exists.\n",
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: sentence_length\n",
      "Classes: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 782/782 [04:08<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.9147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 782/782 [03:08<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.7915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 782/782 [03:09<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.7648\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 79/79 [00:18<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe Accuracy: 64.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6491596638655462"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK = \"sentence_length\"\n",
    "\n",
    "data_path = download_probing_data(TASK)\n",
    "tokenizer, encoder = get_bert_encoder()\n",
    "\n",
    "train_ds = ProbingDataset(data_path, 'tr', tokenizer)\n",
    "test_ds = ProbingDataset(data_path, 'te', tokenizer)\n",
    "\n",
    "# Sync label maps (just in case)\n",
    "test_ds.label_map = train_ds.label_map \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Classes: {train_ds.label_map}\")\n",
    "\n",
    "probe = ProbingClassifier(768, HIDDEN_DIM, train_ds.num_classes)\n",
    "\n",
    "train_probe(encoder, probe, train_loader, DEVICE, epochs=EPOCHS)\n",
    "\n",
    "evaluate_probe(encoder, probe, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be859fcb",
   "metadata": {},
   "source": [
    ".6491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf191c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for tree_depth already exists.\n",
      "Loading BERT model...\n",
      "Task: tree_depth\n",
      "Classes: {'10': 0, '11': 1, '5': 2, '6': 3, '7': 4, '8': 5, '9': 6}\n",
      "Starting training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 782/782 [03:06<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.6926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 782/782 [03:04<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 782/782 [03:04<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6092\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 79/79 [00:18<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe Accuracy: 29.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.299"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK = \"tree_depth\"\n",
    "\n",
    "data_path = download_probing_data(TASK)\n",
    "tokenizer, encoder = get_bert_encoder()\n",
    "\n",
    "train_ds = ProbingDataset(data_path, 'tr', tokenizer)\n",
    "test_ds = ProbingDataset(data_path, 'te', tokenizer)\n",
    "\n",
    "# Sync label maps (just in case)\n",
    "test_ds.label_map = train_ds.label_map \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Classes: {train_ds.label_map}\")\n",
    "\n",
    "probe = ProbingClassifier(768, HIDDEN_DIM, train_ds.num_classes)\n",
    "\n",
    "train_probe(encoder, probe, train_loader, DEVICE, epochs=EPOCHS)\n",
    "\n",
    "evaluate_probe(encoder, probe, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03dc7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
